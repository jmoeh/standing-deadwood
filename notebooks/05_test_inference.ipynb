{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, Subset, RandomSampler\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "from torchmetrics import Dice\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.dirname(os.path.realpath(os.path.abspath(\"\"))))\n",
    "\n",
    "from unet.dataset import DeadwoodDataset\n",
    "from unet.dice_score import dice_coeff, confusion_values, confusion_tensor\n",
    "from unet.evaluate import evaluate\n",
    "from unet.unet_model import UNet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"garg20i1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(sys.modules['unet.dice_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "experiment = api.run(f\"jmoehring/standing-deadwood-unet-pro/{run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_val_samples: int = 0\n",
    "epoch_models = [19, 12, 20]\n",
    "\n",
    "fold: int = 2\n",
    "epoch: int = epoch_models[fold]\n",
    "\n",
    "# data paths\n",
    "checkpoint_dir = f\"/net/scratch/jmoehring/checkpoints/{experiment.name}\"\n",
    "model_checkpoint = f\"fold_{fold}_epoch_{epoch}.pth\"\n",
    "\n",
    "# data params\n",
    "no_folds: int = experiment.config[\"data\"][\"no_folds\"]\n",
    "random_seed: int = experiment.config[\"data\"][\"random_seed\"]\n",
    "batch_size: int = experiment.config[\"data\"][\"batch_size\"]\n",
    "test_size: float = experiment.config[\"data\"][\"test_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_df = pd.read_csv(experiment.config[\"data\"][\"register_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DeadwoodDataset(\n",
    "    register_df=register_df,\n",
    "    no_folds=no_folds,\n",
    "    random_seed=random_seed,\n",
    "    test_size=test_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_args = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 12,\n",
    "    \"pin_memory\": True,\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "_, val_set = dataset.get_train_val_fold(fold)\n",
    "val_loader = DataLoader(val_set, **loader_args)\n",
    "\n",
    "# only sample a subset of the validation set\n",
    "if no_val_samples > 0:\n",
    "    loader_args[\"shuffle\"] = False\n",
    "    sampler = RandomSampler(val_set, num_samples=no_val_samples)\n",
    "    val_loader.sampler = sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preferably use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_ids = [0]\n",
    "# model with three input channels (RGB)\n",
    "model = UNet(n_channels=3, n_classes=1, bilinear=True)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    device_ids = [0, 1]\n",
    "model = nn.DataParallel(model, device_ids=device_ids)\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, model_checkpoint)))\n",
    "model = model.to(memory_format=torch.channels_last, device=device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"{experiment.name}_fold_{fold}_epoch_{epoch}_eval\"\n",
    "wandb.init(project=\"standing-deadwood-unet-pro\", name=run_name, resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 0\n",
    "\n",
    "eval_df = pd.DataFrame(columns=[\"biome\", \"resolution_bin\", \"precision\", \"recall\", \"f1\"])\n",
    "for batch, (images, true_masks, images_metas) in tqdm(\n",
    "    enumerate(val_loader), total=len(val_loader)\n",
    "):\n",
    "    images = images.to(memory_format=torch.channels_last, device=device)\n",
    "    true_masks = true_masks.to(device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_masks = model(images)\n",
    "        pred_masks = torch.sigmoid(pred_masks)\n",
    "        pred_masks_sig = (pred_masks > 0.5).float()\n",
    "        precision, recall, f1 = confusion_values(\n",
    "            pred_masks_sig.squeeze(), true_masks.squeeze()\n",
    "        )\n",
    "        # extend the dataframe by all the results of the batch\n",
    "        for i in range(len(images)):\n",
    "            eval_row = pd.DataFrame(\n",
    "                {\n",
    "                    \"biome\": [images_metas[\"biome\"][i].item()],\n",
    "                    \"resolution_bin\": [images_metas[\"resolution_bin\"][i].item()],\n",
    "                    \"precision\": [precision[i].numpy()],\n",
    "                    \"recall\": [recall[i].numpy()],\n",
    "                    \"f1\": [f1[i].numpy()],\n",
    "                },\n",
    "            )\n",
    "            eval_df = pd.concat([eval_df, eval_row])\n",
    "            # if image_count < 50 and true_masks[i].sum() > 0:\n",
    "            #     merged_confusion_tensor = confusion_tensor(\n",
    "            #         pred_masks_sig[i].squeeze(), true_masks[i].squeeze()\n",
    "            #     )\n",
    "            #     wandb.log(\n",
    "            #         {\n",
    "            #             \"segmentation\": wandb.Image(\n",
    "            #                 images[i].float().cpu(),\n",
    "            #                 masks={\n",
    "            #                     \"confusion\": {\n",
    "            #                         \"mask_data\": merged_confusion_tensor.float()\n",
    "            #                         .cpu()\n",
    "            #                         .squeeze()\n",
    "            #                         .numpy(),\n",
    "            #                         \"class_labels\": {\n",
    "            #                             1: \"true_positive\",\n",
    "            #                             2: \"false_positive\",\n",
    "            #                             3: \"false_negative\",\n",
    "            #                         },\n",
    "            #                     },\n",
    "            #                 },\n",
    "            #             )\n",
    "            #         }\n",
    "            #     )\n",
    "            #     image_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"f1\"] = eval_df[\"f1\"].astype(float)\n",
    "eval_df[\"precision\"] = eval_df[\"precision\"].astype(float)\n",
    "eval_df[\"recall\"] = eval_df[\"recall\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(\n",
    "    f\"/net/scratch/jmoehring/eval_{experiment.name}_fold_{fold}_epoch_{epoch}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_df = pd.read_csv(\n",
    "#     f\"/net/scratch/jmoehring/eval_{experiment.name}_fold_{fold}_epoch_{epoch}.csv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column biome names\n",
    "biome_names = {\n",
    "    4: \"Temperate Broadleaf and Mixed Forests\",\n",
    "    5: \"Temperate Coniferous Forests\",\n",
    "    6: \"Boreal Forests/Taiga\",\n",
    "    12: \"Mediteranean Forests\",\n",
    "}\n",
    "eval_df[\"biome_name\"] = eval_df[\"biome\"].map(biome_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot seaborn heatmap of dice scores with biome and resolution as x and y axis\n",
    "sns.set_theme()\n",
    "pivoted = eval_df.pivot_table(index=\"biome_name\", columns=\"resolution_bin\", values=\"f1\")\n",
    "sns.heatmap(pivoted, cmap=\"rocket\", annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
